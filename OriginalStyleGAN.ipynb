{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5bc00c5-1d59-43a2-9457-9ecf53e18fc9",
   "metadata": {},
   "source": [
    "# Neural Style GAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "681f09cb-95d3-4638-b00b-2ca6e2b152a8",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4e4d099-43ec-47e3-af42-8eff25913d25",
   "metadata": {},
   "source": [
    "StyleGAN has been introduced by [Leon Gatys et al.](https://www.researchgate.net/publication/281312423_A_Neural_Algorithm_of_Artistic_Style) in 2015. The neural style transfer algorithm has undergone many refinements and spawned many variations since its original introduction. It consists of applying the style of a reference image to a target image while conserving the content of the target image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4036051a-e622-43ea-9feb-f57dc1e9b599",
   "metadata": {},
   "source": [
    "![exemple](example.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d51da9fa-63d3-4899-a9fc-79b325bb8271",
   "metadata": {},
   "source": [
    "In this context, style essentially means *textures*, *colors*, and *visual patterns* in the image, at various *spatial scales*, and the content is the **higher-level macrostructure of the image**.\n",
    "\n",
    "We want to conserve the content of the original image while adopting the style of the reference image. If we were able to mathematically define content and style, then an appropriate loss function to minimize would be the following:\n",
    "\n",
    "`loss = (distance(style(reference_image) - style(combination_image)) + distance(content(original_image) - content(combination_image)))`\n",
    "\n",
    "Here, distance is a norm function such as the $L2$ norm, content is a function that takes an image and computes a representation of its content, and style is a function that takes an image and computes a representation of its style. Minimizing this loss causes `style(combination_image)` to be close to `style(reference_image)`, and `content(combination_image)` is close to `content(original_image)`, thus achieving\n",
    "style transfer as we defined it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32c6cd92-0cdf-4698-85ba-5cfefffd794b",
   "metadata": {},
   "source": [
    "## The content loss\n",
    "\n",
    "As you already know, activations from earlier layers in a network contain local information about the image, whereas activations from higher layers contain increasingly global, abstract information. Formulated in a different way, the activations of the different layers of a convnet provide a decomposition of the contents of an image over different spatial scales. Therefore, you’d expect the content of an image, which is more global and abstract, to be captured by the representations of the upper layers in a convnet. A good candidate for content loss is thus the L2 norm between the activations of an upper layer in a pretrained convnet, computed over the target image, and the activations of the same layer computed over the generated image. This guarantees that, as seen from the upper layer, the generated image will look similar to the original target image. Assuming that what the upper layers of a convnet see is really the content of their input images, this works as a way to preserve image content."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80adfbfd-62ea-420d-99ab-0a731d8da14e",
   "metadata": {},
   "source": [
    "## The style loss\n",
    "\n",
    "The content loss only uses a single upper layer, but the style loss as defined by Gatys et al. uses multiple layers of a convnet: you try to capture the appearance of the style-reference image at all spatial scales extracted by the convnet, not just a single scale.\n",
    "\n",
    "For the style loss, Gatys et al. use the Gram matrix of a layer’s activations: the inner product of the feature maps of a given layer. This inner product can be understood as representing a map of the correlations between the layer’s features. These feature correlations capture the statistics of the patterns of a particular spatial scale, which empirically correspond to the appearance of the textures found at this scale. Hence, the style loss aims to preserve similar internal correlations within the activations of different layers, across the style-reference image and the generated image. In turn, this guarantees that the textures found at different spatial scales look similar across the style-reference image and the generated image.\n",
    "\n",
    "In short, you can use a pretrained convnet to define a loss that will do the following:\n",
    "* Preserve content by maintaining similar high-level layer activations between the original image and the generated image. The convnet should “see” both the original image and the generated image as containing the same things.\n",
    "* Preserve style by maintaining similar correlations within activations for both low-level layers and high-level layers. Feature correlations capture textures: the generated image and the style-reference image should share the same textures at different spatial scales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "930e3356-1898-48c2-a13f-d8fceaf95095",
   "metadata": {},
   "source": [
    "## Neural style transfer in Keras\n",
    "\n",
    "Neural style transfer can be implemented using any pretrained convnet. Here, we’ll use the VGG19 network used by Gatys et al. Here’s the general process:\n",
    "* Set up a network that computes VGG19 layer activations for the style-reference image, the base image, and the generated image at the same time.\n",
    "* Use the layer activations computed over these three images to define the loss function described earlier, which we’ll minimize in order to achieve style transfer.\n",
    "* Set up a gradient-descent process to minimize this loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bdc6454-4adf-42fd-a277-b13f3106e719",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05af5d2-562e-4b2f-89a0-4a3d02cc9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image_path = \"./random_face_1.png\"\n",
    "style_reference_image_path = \"./style_klimt.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304fcebf-3de2-4615-b629-f3b71a948a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_width, original_height = keras.utils.load_img(base_image_path).size\n",
    "img_height = 700\n",
    "img_width = round(original_width * img_height / original_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4deb9c17-97c0-4342-916f-211735c5a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = keras.utils.load_img(image_path, \n",
    "                               target_size=(img_height, img_width))\n",
    "    img = keras.utils.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = keras.applications.vgg19.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7221e596-41f1-4aea-b537-c569d2c86e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(img):\n",
    "    img = img.reshape((img_height, img_width, 3))\n",
    "    # Zero-centering by removing the mean pixel value from imagenet. \n",
    "    # This reverses a transformation done by vgg19.preprocess_input\n",
    "    img[:, :, 0] += 103.939\n",
    "    img[:, :, 1] += 116.779\n",
    "    img[:, :, 2] += 123.68\n",
    "    # Converts images from \"BGR\" to \"RGB\". \n",
    "    # This is also part of the revsersal vgg19.preprocess_input\n",
    "    img = img[:, :, ::-1]\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b2d958-d2ee-476a-adb3-2701beeb4546",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69352f6c-2d07-44cb-935f-7e1db924510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base_img, combination_img):\n",
    "    return tf.reduce_sum(tf.square(combination_img - base_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc945c95-e2f1-4b57-93eb-0e12ea22d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style_img, combination_img):\n",
    "    S = gram_matrix(style_img)\n",
    "    C = gram_matrix(combination_img)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d989b5-717e-48e4-b444-a600e5bbfae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = tf.square(\n",
    "        x[:, : img_height - 1, : img_width - 1, :] - x[:, 1:, : img_width - 1, :]\n",
    "    )\n",
    "    b = tf.square(\n",
    "        x[:, : img_height - 1, : img_width - 1, :] - x[:, : img_height - 1, 1:, :]\n",
    "    )\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d903756b-298a-4a2b-9f07-25300c7cfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layer_names = [\"block1_conv1\",\n",
    "                     \"block2_conv1\",\n",
    "                     \"block3_conv1\",\n",
    "                     \"block4_conv1\",\n",
    "                     \"block5_conv1\",]\n",
    "\n",
    "content_layer_name = \"block5_conv2\"\n",
    "\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ad4d0b4-ad76-416e-92ae-3cea184fdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    "    input_tensor = tf.concat([base_image, \n",
    "                              style_reference_image, \n",
    "                              combination_image], \n",
    "                             axis=0)\n",
    "    features = feature_extractor(input_tensor)\n",
    "    loss = tf.zeros(shape=())\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss = loss + content_weight * content_loss(base_image_features, \n",
    "                                                combination_features)\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        style_loss_value = style_loss(style_reference_features, \n",
    "                                      combination_features)\n",
    "        loss += (style_weight / len(style_layer_names)) * style_loss_value\n",
    "\n",
    "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd63c945-5980-4495-afe1-6e87b0419ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss_and_grads(\n",
    "    combination_image, base_image, style_reference_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, \n",
    "                            base_image, \n",
    "                            style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4cf1da7-5dea-431a-9394-072656b803d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(\n",
    "    keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=100.0, \n",
    "        decay_steps=100, \n",
    "        decay_rate=0.96)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1627ac8a-f60c-40bf-be7b-644e18e68ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image = preprocess_image(base_image_path)\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "combination_image = tf.Variable(preprocess_image(base_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65274074-2fe6-4faf-8ec5-00eb74bc4c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: loss=2154.66\n",
      "Iteration 200: loss=1560.62\n",
      "Iteration 300: loss=1312.12\n",
      "Iteration 400: loss=1168.43\n",
      "Iteration 500: loss=1072.89\n",
      "Iteration 600: loss=1003.91\n",
      "Iteration 700: loss=951.24\n",
      "Iteration 800: loss=909.61\n",
      "Iteration 900: loss=875.64\n",
      "Iteration 1000: loss=847.34\n",
      "Iteration 1100: loss=823.39\n",
      "Iteration 1200: loss=802.88\n",
      "Iteration 1300: loss=785.01\n",
      "Iteration 1400: loss=769.34\n",
      "Iteration 1500: loss=755.41\n",
      "Iteration 1600: loss=742.97\n",
      "Iteration 1700: loss=731.80\n",
      "Iteration 1800: loss=721.77\n",
      "Iteration 1900: loss=712.65\n",
      "Iteration 2000: loss=704.34\n"
     ]
    }
   ],
   "source": [
    "iterations = 2000\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    for i in range(1, iterations + 1):\n",
    "        loss, grads = compute_loss_and_grads(\n",
    "            combination_image, \n",
    "            base_image, \n",
    "            style_reference_image\n",
    "        )\n",
    "        optimizer.apply_gradients([(grads, combination_image)])\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: loss={loss:.2f}\")\n",
    "            img = deprocess_image(combination_image.numpy())\n",
    "            fname = f\"combination_image_at_iteration_{i}.png\"\n",
    "            keras.utils.save_img(fname, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3e416-78b5-4826-a5ee-d1fafea4f13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38af49-c101-42b1-87d0-aec76ba1e848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
